{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e648c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import re\n",
    "from multObjGenContext import *\n",
    "from multObjGenFunctions import *\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e98774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_vram_usage(stage):\n",
    "    allocated = torch.cuda.memory_allocated()/(1024 ** 3)  \n",
    "    reserved = torch.cuda.memory_reserved()/(1024 ** 3)  \n",
    "    print(f\"[{stage}] VRAM Usage - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "def log_peak_vram_usage(stage):\n",
    "    peak = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\n[Peak {stage} usage] VRAM usage - Peak: {peak:.2f} GB\")\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.ipc_collect()  \n",
    "    gc.collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd5dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\",\n",
    "    #\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    #\"TheBloke/deepseek-coder-1.3b-instruct-GPTQ\",\n",
    "    #\"TechxGenus/gemma-2b-GPTQ\",\n",
    "    #\"TheBloke/stable-code-3b-GPTQ\",\n",
    "    #\"TheBloke/phi-2-GPTQ\",\n",
    "    #\"TheBloke/phi-2-orange-GPTQ\",\n",
    "    #\"TheBloke/deepseek-llm-7B-base-GPTQ\",\n",
    "    #\"TheBloke/llama-deus-7b-v3-GPTQ\",\n",
    "    #\"TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ\",\n",
    "    #\"TheBloke/Llama-2-7b-Chat-GPTQ\",\n",
    "    #\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\",\n",
    "    #\"TechxGenus/Meta-Llama-3-8B-GPTQ\",\n",
    "    #\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\",\n",
    "    #\"TheBloke/LLaMA-Pro-8B-GPTQ\"\n",
    "\n",
    "]\n",
    "\n",
    "test_cases_object_list = [\n",
    "    \"Complete a kitchen setup\",\n",
    "    \"Complete batman's utility belt\",\n",
    "    \"Complete american car set\",\n",
    "    \"Complete a medieval weapons set\",\n",
    "    \"Complete a bedroom setup\",\n",
    "    \"Complete a fruit bowl\",\n",
    "    \"Complete a pirate ship deck\",\n",
    "    \"Complete a modern living room setup\",\n",
    "    \"Complete a medieval marketplace\",\n",
    "    \"Complete a science lab workspace\"\n",
    "]\n",
    "\n",
    "test_cases_spatial = [\n",
    "    [\"keyboard\", \"monitor\", \"mouse\", \"pc\"],\n",
    "    [\"guitar\", \"amplifier\", \"microphone\", \"music stand\"],\n",
    "    [\"pan\", \"stove\", \"spatula\", \"cutting board\"],\n",
    "    [\"helmet\", \"armor\", \"sword\", \"shield\"],\n",
    "    [\"camera\", \"tripod\", \"lens\", \"flash\"],\n",
    "    [\"tree\", \"bench\", \"fountain\", \"lamp post\"],\n",
    "    [\"basketball\", \"hoop\", \"scoreboard\", \"bleachers\"],\n",
    "    [\"train\", \"track\", \"station\", \"ticket booth\"],\n",
    "    [\"painting\", \"easel\", \"paintbrush\", \"palette\"],\n",
    "    [\"fish tank\", \"filter\", \"air pump\", \"plants\"],\n",
    "]\n",
    "\n",
    "test_cases_coordinates = [\n",
    "    [\"keyboard infrontof monitor\", \"mouse totherightof keyboard\", \"pc totheleftof monitor\"],\n",
    "    [\"guitar infrontof amplifier\", \"microphone totherightof guitar\", \"music stand totheleftof guitar\"],\n",
    "    [\"pan infrontof stove\", \"spatula totherightof stove\", \"cutting board totheleftof stove\"],\n",
    "    [\"helmet infrontof armor\", \"sword totherightof helmet\", \"shield totheleftof armor\"],\n",
    "    [\"camera infrontof tripod\", \"lens totherightof camera\", \"flash totheleftof camera\"],\n",
    "    [\"tree infrontof bench\", \"bench totherightof tree\", \"fountain totheleftof bench\"],\n",
    "    [\"basketball infrontof hoop\", \"hoop totherightof scoreboard\", \"scoreboard totheleftof bleachers\"],\n",
    "    [\"train infrontof station\", \"track totherightof train\", \"ticket booth totheleftof station\"],\n",
    "    [\"painting infrontof easel\", \"paintbrush totherightof painting\", \"palette totheleftof easel\"],\n",
    "    [\"fish tank infrontof filter\", \"filter totherightof fish tank\", \"air pump totheleftof filter\"]\n",
    "]\n",
    "\n",
    "output_file = \"evaluation_results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b27f605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\n",
      "[Before TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ load] VRAM Usage - Allocated: 0.00 GB, Reserved: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After model load] VRAM Usage - Allocated: 0.73 GB, Reserved: 0.78 GB\n",
      "-Testing Object List Assistant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object List Assistant finished in 43.34 seconds\n",
      "\n",
      "-Testing Relational Mapping Assistant\n",
      "Relational Mapping Assistant finished in 38.40 seconds\n",
      "\n",
      "-Testing Grid Placement Assistant\n",
      "Grid Placement Assistant finished in 31.78 seconds\n",
      "\n",
      "[After all tests] VRAM Usage - Allocated: 0.74 GB, Reserved: 1.09 GB\n",
      "[Peak VRAM] TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ: 0.93 GB\n",
      "\n",
      "Writing...\n",
      "Finished testing TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Testing model: {model_name}\")\n",
    "    clear_memory()  \n",
    "    log_vram_usage(f\"Before {model_name} load\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", trust_remote_code=True, revision=\"main\")    # trust has to be set to true for some models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    log_vram_usage(\"After model load\")\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "    object_list_assistant = ObjectListAssistant(pipe, objlist_context)\n",
    "    spatial_assistant = RelationalMappingAssistant(pipe, relational_context)\n",
    "    coordinate_assistant = GridPlacementAssistant(pipe, grid_context)\n",
    "    \n",
    "    model_results = []\n",
    "    model_results.append(f\"========== Model: {model_name} ==========\\n\")\n",
    "\n",
    "    print(f\"-Testing Object List Assistant\")\n",
    "    start_time = time.time()\n",
    "    model_results.append(\"object_list_test:\\n\")\n",
    "    for prompt in test_cases_object_list:\n",
    "        result = object_list_assistant.process_request(prompt)\n",
    "        model_results.append(f\"Prompt: {prompt}\\nResult: {result}\\n\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    timestamp = time.time()\n",
    "    print(f\"Object List Assistant finished in {elapsed:.2f} seconds\\n\")\n",
    "\n",
    "    print(f\"-Testing Relational Mapping Assistant\")\n",
    "    model_results.append(\"\\nrelational_test:\\n\")\n",
    "    for objects in test_cases_spatial:\n",
    "        result = spatial_assistant.process_request(objects)\n",
    "        model_results.append(f\"Objects: {objects}\\nResult: {result}\\n\")\n",
    "\n",
    "    elapsed = time.time() - timestamp\n",
    "    timestamp = time.time()\n",
    "    print(f\"Relational Mapping Assistant finished in {elapsed:.2f} seconds\\n\")\n",
    "    \n",
    "\n",
    "    print(f\"-Testing Grid Placement Assistant\")\n",
    "    model_results.append(\"\\ngrid_test:\\n\")\n",
    "    for relations in test_cases_coordinates:\n",
    "        result = coordinate_assistant.process_request(relations)\n",
    "        model_results.append(f\"Relations: {relations}\\nResult: {result}\\n\")\n",
    "    \n",
    "    elapsed = time.time() - timestamp\n",
    "    print(f\"Grid Placement Assistant finished in {elapsed:.2f} seconds\\n\")\n",
    "\n",
    "    log_vram_usage(\"After all tests\")\n",
    "\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    peak_log = f\"[Peak VRAM] {model_name}: {peak_memory:.2f} GB\\n\"\n",
    "\n",
    "    print(f'{peak_log}')\n",
    "    \n",
    "    model_results.append(\"\\n============================================\\n\\n\")\n",
    "    \n",
    "    with open(output_file, \"a\") as f:\n",
    "        print(\"Writing...\")\n",
    "        f.writelines(model_results)\n",
    "    \n",
    "    del model\n",
    "    del tokenizer\n",
    "    del pipe\n",
    "    clear_memory()\n",
    "    \n",
    "    print(f\"Finished testing {model_name}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
