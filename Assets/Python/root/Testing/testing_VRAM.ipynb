{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637906e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline as hf_pipeline\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from diffusers import StableDiffusionPipeline, ShapEPipeline\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1bdde76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_vram_usage(stage):\n",
    "    allocated = torch.cuda.memory_allocated()/(1024 ** 3)  \n",
    "    reserved = torch.cuda.memory_reserved()/(1024 ** 3)  \n",
    "    print(f\"[{stage}] VRAM Usage - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "def log_peak_vram_usage(stage):\n",
    "    peak = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\n[Peak {stage} usage] VRAM usage - Peak: {peak:.2f} GB\")\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.ipc_collect()  \n",
    "    gc.collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12bdcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_prompts = [\n",
    "    \"wooden chair with curved legs\",\n",
    "    \"futuristic drone\",\n",
    "    \"fantasy sword\",\n",
    "    \"toy robot\",\n",
    "    \"detailed coffee mug\",\n",
    "    \"space helmet\",\n",
    "    \"medieval lantern\",\n",
    "    \"pirate ship cannon\",\n",
    "    \"mountain bike\"\n",
    "]\n",
    "\n",
    "sd_prompts = [\n",
    "    \"futuristic city at sunset\",\n",
    "    \"medieval castle on a mountain\",\n",
    "    \"robot in a field of flowers\",\n",
    "    \"cyberpunk street at night\",\n",
    "    \"astronaut relaxing on the moon\",\n",
    "    \"mystical forest with glowing trees\",\n",
    "    \"dragon flying over a volcano\",\n",
    "    \"fantasy village beside a waterfall\",\n",
    "    \"spaceship interior with glowing panels\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9aa560",
   "metadata": {},
   "source": [
    "Note: run each one individully and then restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead72748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before loading Shap-E] VRAM Usage - Allocated: 0.00 GB, Reserved: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "shap_e_renderer\\diffusion_pytorch_model.safetensors not found\n",
      "The config attributes {'renderer': ['shap_e', 'ShapERenderer']} were passed to ShapEPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
      "Keyword arguments {'renderer': ['shap_e', 'ShapERenderer']} are not expected by ShapEPipeline and will be ignored.\n",
      "Loading pipeline components...:  20%|██        | 1/5 [00:00<00:02,  1.63it/s]c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After loading Shap-E] VRAM Usage - Allocated: 1.25 GB, Reserved: 1.26 GB\n",
      "\n",
      "Generating 3D object 1/10: wooden chair with curved legs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:07<00:00, 18.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D object 2/10: futuristic drone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:06<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D object 3/10: fantasy sword\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:06<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D object 4/10: toy robot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:06<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D object 5/10: detailed coffee mug\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:06<00:00, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D object 6/10: space helmet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:06<00:00, 19.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D object 7/10: medieval lantern\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:07<00:00, 17.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D object 8/10: pirate ship cannon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:07<00:00, 17.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D object 9/10: mountain bike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:06<00:00, 19.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After generating with Shap-E] VRAM Usage - Allocated: 1.26 GB, Reserved: 2.49 GB\n",
      "\n",
      "[Peak Shap-E usage] VRAM usage - Peak: 2.22 GB\n"
     ]
    }
   ],
   "source": [
    "log_vram_usage(\"Before loading Shap-E\")\n",
    "shape_pipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "shape_pipe = shape_pipe.to(\"cuda\")\n",
    "log_vram_usage(\"After loading Shap-E\")\n",
    "\n",
    "for i, prompt in enumerate(shape_prompts, 1):\n",
    "    print(f\"\\nGenerating 3D object {i}/10: {prompt}\")\n",
    "    _ = shape_pipe(prompt, guidance_scale=30, num_inference_steps=64, frame_size=256, output_type=\"mesh\").images[0]\n",
    "\n",
    "log_vram_usage(\"After generating with Shap-E\")\n",
    "log_peak_vram_usage(\"Shap-E\")\n",
    "\n",
    "del shape_pipe\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8746a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before loading Stable Diffusion] VRAM Usage - Allocated: 0.01 GB, Reserved: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "safety_checker\\model.safetensors not found\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:03,  1.96it/s]c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After loading Stable Diffusion] VRAM Usage - Allocated: 2.58 GB, Reserved: 2.64 GB\n",
      "\n",
      "Generating image 1/10: futuristic city at sunset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating image 2/10: medieval castle on a mountain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating image 3/10: robot in a field of flowers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating image 4/10: cyberpunk street at night\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating image 5/10: astronaut relaxing on the moon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating image 6/10: mystical forest with glowing trees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating image 7/10: dragon flying over a volcano\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating image 8/10: fantasy village beside a waterfall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating image 9/10: spaceship interior with glowing panels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After generating with Stable Diffusion] VRAM Usage - Allocated: 2.58 GB, Reserved: 3.56 GB\n",
      "\n",
      "[Peak Stable Diffusion usage] VRAM usage - Peak: 3.34 GB\n"
     ]
    }
   ],
   "source": [
    "log_vram_usage(\"Before loading Stable Diffusion\")\n",
    "sd_pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)\n",
    "sd_pipe = sd_pipe.to(\"cuda\")\n",
    "log_vram_usage(\"After loading Stable Diffusion\")\n",
    "\n",
    "for i, prompt in enumerate(sd_prompts, 1):\n",
    "    print(f\"\\nGenerating image {i}/10: {prompt}\")\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        _ = sd_pipe(prompt).images[0]\n",
    "\n",
    "log_vram_usage(\"After generating with Stable Diffusion\")\n",
    "log_peak_vram_usage(\"Stable Diffusion\")\n",
    "\n",
    "del sd_pipe\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e25f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before loading TinyLlama] VRAM Usage - Allocated: 0.01 GB, Reserved: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "c:\\Users\\toer2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After loading TinyLlama] VRAM Usage - Allocated: 0.74 GB, Reserved: 0.78 GB\n"
     ]
    }
   ],
   "source": [
    "log_vram_usage(\"Before loading TinyLlama\")\n",
    "\n",
    "model_name_or_path = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, device_map=\"cuda\", trust_remote_code=True, revision=\"main\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "log_vram_usage(\"After loading TinyLlama\") \n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "clear_memory() # doesn't clear VRAM for LLMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baad160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before loading Llama 2] VRAM Usage - Allocated: 0.74 GB, Reserved: 0.78 GB\n",
      "[After loading Llama 2] VRAM Usage - Allocated: 4.48 GB, Reserved: 4.77 GB\n"
     ]
    }
   ],
   "source": [
    "log_vram_usage(\"Before loading Llama 2\")\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, device_map=\"cuda\", trust_remote_code=True, revision=\"main\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "log_vram_usage(\"After loading Llama 2\") \n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
