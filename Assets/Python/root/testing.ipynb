{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, Pipeline\n",
    "import torch\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"cuda\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, size: int) -> None:\n",
    "        self.queue = Queue(size)\n",
    "\n",
    "    def add(self, item):\n",
    "        if self.queue.full(): self.queue.get()\n",
    "        self.queue.put(item)\n",
    "    \n",
    "    def all(self):\n",
    "        return [i for i in self.queue.queue]\n",
    "    \n",
    "    def last(self):\n",
    "        return self.all()[-1]\n",
    "    def previous(self):\n",
    "        return '\\n'.join(self.all()[:-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Buffer(3)\n",
    "b.add(5)\n",
    "b.add(4)\n",
    "b.add(3)\n",
    "b.add(2)\n",
    "b.previous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPC:\n",
    "    def __init__(self, name, desc, model, tokenizer) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.name = name\n",
    "        self.desc = desc\n",
    "        self.memory = Buffer(20)\n",
    "        self.chat = pipeline(\n",
    "            \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.95, top_k=40, repetition_penalty=1.5\n",
    "        )\n",
    "        self.initiative = pipeline(\n",
    "            \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10, do_sample=True, temperature=0.7, top_p=0.95, top_k=40, repetition_penalty=1.5\n",
    "        )\n",
    "        \n",
    "    #called on NPCs in earshot\n",
    "    def hear(self, message):\n",
    "        self.memory.add(message)\n",
    "        return message\n",
    "\n",
    "    def test_initiative(self) -> bool:\n",
    "        answer: str = self.initiative(self.initiative_template())[0]['generated_text'].split(\"[/INST]\\n\")[1].strip()\n",
    "        return \"YES\" in answer.upper()\n",
    "        \n",
    "    def respond(self) -> str:\n",
    "        if self.test_initiative():\n",
    "            response = self.chat(self.chat_template())[0]['generated_text'].split(\"[/INST]\\n\")[1].split(\"\\n\")[0].strip()\n",
    "            self.memory.add(response)\n",
    "            return response\n",
    "\n",
    "    def chat_template(self):\n",
    "        return f\"\"\"\n",
    "            [INST] <<SYS>>\n",
    "            You are {self.name}: {self.desc}. \n",
    "            Keep your responses brief, 1-2 sentences.\n",
    "            Only respond as {self.name}\n",
    "            Here is the chat history\n",
    "            {self.memory.previous()}\n",
    "            <</SYS>>\n",
    "            {self.memory.last()}\n",
    "            [/INST]\n",
    "            {self.name}:\"\"\"\n",
    "    \n",
    "    def initiative_template(self):\n",
    "        return f\"\"\"\n",
    "            [INST] <<SYS>>\n",
    "            You are {self.name}: {self.desc}. \n",
    "            Should you respond at this point in the conversation?\n",
    "            Answer with YES or NO in this format:\n",
    "            Answer: YES\n",
    "            Here is the chat history\n",
    "            {self.memory.previous()}\n",
    "            <</SYS>>\n",
    "            {self.memory.last()}\n",
    "            [/INST]\n",
    "            Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: YES\n",
      "Batman: (gritting teeth) Oh, great. What's your little brain-twister this time, Riddler?\n",
      "Batman: (sighing) To get away from you, Riddler. Now if you'll excuse me, I have real crime to deal with.\n"
     ]
    }
   ],
   "source": [
    "batman = NPC(\"Batman\", \"A caped crusader, who prowls the streets of gotham keeping the city safe from criminals.\", model, tokenizer)\n",
    "batman.hear(\"Riddler: Hello Batman! Want to hear a riddle?\")\n",
    "print(batman.test_initiative())\n",
    "print(batman.respond())\n",
    "batman.hear(\"Riddler\", \"So Batman... Why did the chicken cross the road?\")\n",
    "print(batman.respond())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Riddler: Hello Batman! Want to hear a riddle?',\n",
       " \"Batman: (gritting teeth) Oh, great. What's your little brain-twister this time, Riddler?\",\n",
       " 'Riddler: So Batman... Why did the chicken cross the road?',\n",
       " \"Batman: (sighing) To get away from you, Riddler. Now if you'll excuse me, I have real crime to deal with.\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batman.memory.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Batman: Is that you Riddler!!?\n",
      " | Riddler: Ah, Batman! Always so quick to answer my questions... but can he solve mine? *winks*\n",
      " | Batman: (grimacing) Don't play games with me,Riddlemeier. The people of Gotham need protection and I won't be distracted by your wordplay.\n",
      " | User: Is that the joker attacking a civilian!!!\n",
      " | Riddler: Oh, just another innocent victim for our little game of cat and mouse... ^_^\n",
      " | Batman: *pauses*, Where? Now?\n",
      " | Riddler: *giggling maniacally* Why, everywhere, dear Dark Knight! Everywhere! The streets of this fair city run red with civilians prey for us to hunt...and feast upon their fears **evil laugh**. But don't worry; we have no intention of harm them yet they will suffer nonetheless from Our Grand Game Of Life And Death \\o/. Do tell,Our Dearest Foes what shall Their Fate Become? Will it be TickleMe or Screaming SillyWilly? Or perhaps something far more sinister like Jigsaw\n",
      " | Batman: (Deep Voice) No time foe indulge in such frivolities now. Civilians safety must come first.I sense multiple hostages taken at various locations around town,We Must act quickly before situation deteriorates further!Where do u think these captives could b located?\n",
      " | Riddler: Hahaahaaa! *chuckles evilly*.Oh,Darknight detective,you really know howto get undermy skin~ *poke poke *.But alasyour haste only servesour purposes well enough. *smirks wickedly.*Those unfortunategood soulsare scattered acrossGothams many hidden lairs& secret strongholds, awaitinyou their turn onthe grand boardof life & death. *cackled madly*.Ahhbutdon'thave anyfearsfor thosecapturedinnocents;theywillbe treatedwith all due\n",
      " | Batman: //Grr// I cannot let you continue tormenting innocence lives, riddlemaster. My duty calls, where would i find theses prisoners location?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 6\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     rr \u001b[39m=\u001b[39m riddler\u001b[39m.\u001b[39;49mrespond()\n\u001b[0;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m rr: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m batman\u001b[39m.\u001b[39mhear(rr))\n",
      "Cell \u001b[1;32mIn[68], line 26\u001b[0m, in \u001b[0;36mNPC.respond\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_initiative():\n\u001b[1;32m---> 26\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_template())[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m[/INST]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     27\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39madd(response)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:219\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39mComplete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m      ids of the generated text.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1162\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1169\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1168\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1169\u001b[0m model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1170\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1067\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1068\u001b[0m model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1069\u001b[0m model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:295\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    296\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1524\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1525\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[0;32m   1526\u001b[0m         input_ids,\n\u001b[0;32m   1527\u001b[0m         logits_processor\u001b[39m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1528\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[0;32m   1529\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1530\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1531\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1532\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1533\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1534\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1535\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1536\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1537\u001b[0m     )\n\u001b[0;32m   1539\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1540\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:2622\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2621\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2622\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2623\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2624\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2625\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2626\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2627\u001b[0m )\n\u001b[0;32m   2629\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1183\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1184\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   1185\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1186\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1187\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1188\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1189\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1190\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1191\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1192\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1193\u001b[0m )\n\u001b[0;32m   1195\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1070\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m   1071\u001b[0m         hidden_states,\n\u001b[0;32m   1072\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1073\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1074\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1075\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1076\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1077\u001b[0m     )\n\u001b[0;32m   1079\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:798\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[0;32m    799\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[0;32m    800\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    801\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m    802\u001b[0m     past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[0;32m    803\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    804\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    805\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    806\u001b[0m )\n\u001b[0;32m    807\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:706\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    704\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39mkv_seq_len)\n\u001b[1;32m--> 706\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[0;32m    708\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:235\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[1;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[0;32m    234\u001b[0m q_embed \u001b[39m=\u001b[39m (q \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_half(q) \u001b[39m*\u001b[39m sin)\n\u001b[1;32m--> 235\u001b[0m k_embed \u001b[39m=\u001b[39m (k \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_half(k) \u001b[39m*\u001b[39m sin)\n\u001b[0;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:208\u001b[0m, in \u001b[0;36mrotate_half\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    207\u001b[0m x2 \u001b[39m=\u001b[39m x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m :]\n\u001b[1;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat((\u001b[39m-\u001b[39;49mx2, x1), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     ur \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUser: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m ur \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m riddler\u001b[39m.\u001b[39mhear(batman\u001b[39m.\u001b[39mhear(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUser: \u001b[39m\u001b[39m{\u001b[39;00mur\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m---> 12\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;49;00m \u001b[39m|\u001b[39;49m \u001b[39mEOFError\u001b[39;49;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'type'"
     ]
    }
   ],
   "source": [
    "batman = NPC(\"Batman\", \"A caped crusader, who prowls the streets of gotham keeping the city safe from criminals.\", model, tokenizer)\n",
    "riddler = NPC(\"Riddler\", \"A supervillan obsessed with riddles and puzzles who torments the citizens of Gotham.\", model, tokenizer)\n",
    "print(\" | \" + riddler.hear(batman.hear(\"Batman: Is that you Riddler!!?\")))\n",
    "while True:\n",
    "    try:\n",
    "        rr = riddler.respond()\n",
    "        if rr: print(\" | \" + batman.hear(rr))\n",
    "        br = batman.respond()\n",
    "        if br: print(\" | \" + riddler.hear(br))\n",
    "        ur = input(\"User: \")\n",
    "        if ur != '': print(\" | \" + riddler.hear(batman.hear(f\"User: {ur}\")))\n",
    "    except KeyboardInterrupt | EOFError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Batman: Is that you Riddler!!?',\n",
       " \"Riddler: Ah, my dear Dark Knight! *chuckle* It's been far too long since we last spoke. How have you been solving my little brain teasers lately? Any progress on unraveling my most recent enigmas?\",\n",
       " \"Batman: (Growling) Don't play games with me, Riddler... I haven't forgotten how to handle a madman like you. Leave Gotham and don't come back or it will be worse for you than Arkham Asylum ever was.\",\n",
       " 'User: I have a riddle. I climb but cannot run. What am I?',\n",
       " 'Riddler: Oh ho! An excellent question, young one! *grinning widely* Are you perhaps an elevator operator? Or maybe someone in charge of moving heavy objects around at work? HAHAHAA! No, no, seriously though, what do YOU think you might be? Go ahead, take another guess if ya want!',\n",
       " 'Batman: Hmmm.... Interesting puzzlement there user... But let us not forget our main goal here - To protect this fair City Of GOTHAM From The Crimson Cowl And His Wretched Minions!!!! We must keep Our Eyes On Penguins And Other Menaces That Threaten Peace In This Holy Land. Now, Please Tell Me More About Yourself So i Can Better Assist U With Ur Query',\n",
       " 'User: Wrong the answer was vines!!',\n",
       " 'Riddler:Ahh,I seeeee🤔Now things get interesting indeedy✨Didnotexpectdatanswerfromyouthereuserpoppet!Butahemnowletmeexplainhowitworksmydearfriend;VINESare afittinganswereforthetask givento thembyour cleverhandymerrymakerbatmannyeveryfinequestionmastermindofdarknessandtrickeryhavefunwiththatonesillygoose(𝘂𝗻�',\n",
       " \"Batman: VINES?! *gasp* Did he just say-- wait, what?! NO ONE EXPECTS THE SPIDER TO SWING INTO THIS CONVERATION LIKE IT'S NORMAL. BUT ME? BATMAN? OF COURSE NOT. *adjusts cowl*\",\n",
       " 'Riddler:Ha haaaaaha💥OhMyGoodnessthespidersonlyJustKeepOnComingbackForMore🕷️Can’twaittoseethisentertainmentContinueplease',\n",
       " 'Batman: Ha! Another foolish mortal thinks they can outsmart the Caped Crusaders, huh? Well, newsflash -- They always lose against their worst nightmares when fighting crime after dark because I make sure justice prevails over evil every time. Briefly express yourself using two sentences only whenever acting as either character mentioned above']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batman.memory.all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
